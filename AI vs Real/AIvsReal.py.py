# -*- coding: utf-8 -*-
"""CYNAPTICS TASK 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gnJB5VoC0hC4pMm2ncMQbEmWymHk-5lE
"""

from google.colab import files
files.upload()  # UploadÂ kaggle.json

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c induction-task

!unzip induction-task.zip

!ls Data

from keras.utils import to_categorical
from keras_preprocessing.image import load_img
from keras.models import Sequential
from keras.applications import MobileNetV2, ResNet152, VGG16, EfficientNetB0, InceptionV3
from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D
import os
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from tqdm.notebook import tqdm
from keras.preprocessing.image import ImageDataGenerator


# def createdataframe(dir):
#     image_paths = []
#     labels = []
#     for label in os.listdir(dir):
#         for imagename in os.listdir(os.path.join(dir, label)):
#             image_paths.append(os.path.join(dir, label, imagename))
#             labels.append(label)
#         print(label, "completed")
#     return image_paths, labels

def createdataframe(dir):
    image_paths = []
    labels = []

    # Check if the directory contains subdirectories or just files
    for item in os.listdir(dir):
        full_path = os.path.join(dir, item)

        if os.path.isdir(full_path):  # If it's a subdirectory
            for imagename in os.listdir(full_path):
                image_paths.append(os.path.join(full_path, imagename))
                labels.append(item)  # Use the subdirectory name as the label
        else:  # If it's a file (e.g., for test data)
            image_paths.append(full_path)
            labels.append("unknown")  # Assign a default label if labels are not available

    print("Dataframe creation completed.")
    return image_paths, labels


# def extract_features(images):
#     features = []
#     for image in tqdm(images):
#         img = load_img(image, target_size=(236, 236))
#         img = np.array(img)
#         features.append(img)
#     features = np.array(features)
#     features = features.reshape(features.shape[0], 236, 236, 3)  # Reshape all images in one go
#     return features

def extract_features(images):
    features = []
    for image in tqdm(images):
        try:
            img = load_img(image, target_size=(236, 236))  # Load the image
            img = np.array(img)  # Convert to numpy array
            features.append(img)  # Add to the features list
        except Exception as e:
            print(f"Error loading image {image}: {e}")  # Log the error
    features = np.array(features)
    features = features.reshape(features.shape[0], 236, 236, 3)  # Reshape all images in one go
    return features


TRAIN_DIR = "Data/Train"

train = pd.DataFrame()
train['image'], train['label'] = createdataframe(TRAIN_DIR)

train_features = extract_features(train['image'])

x_train = train_features / 255.0

le = LabelEncoder()
le.fit(train['label'])
y_train = le.transform(train['label'])
y_train = to_categorical(y_train, num_classes=2)



TEST_DIR="Data/Test"

test=pd.DataFrame()
test['image'],test['label']=createdataframe(TEST_DIR)

# Debugging: Print labels in the test set and the ones learned by LabelEncoder
print("Unique labels in the test set:", test['label'].unique())
print("Labels learned by LabelEncoder:", le.classes_)

# Filter out any unknown labels
valid_labels = le.classes_
test = test[test['label'].isin(valid_labels)]



test_features = extract_features(test['image'])
x_test = test_features / 255.0

y_test = le.transform(test['label'])
y_test = to_categorical(y_test, num_classes=2)





model = Sequential()
# Convolutional layers
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(236, 236, 3)))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(1024, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(2048, activation='relu'))
model.add(Dense(2, activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x=x_train, y=y_train, batch_size=25, epochs=20)

from tensorflow.keras.preprocessing.image import img_to_array

def prepare_test_data(test_dir, target_size=(236, 236)):
    # Get all image files and sort them numerically
    img_files = os.listdir(test_dir)

    # Custom sort key function to handle numerical sorting
    def get_img_number(filename):
        # Extract number from image_X.jpg format
        return int(filename.split('_')[1].split('.')[0])

    # Sort files numerically
    img_files.sort(key=get_img_number)

    test_images = []
    test_ids = []

    # Process images in sorted order
    for img_name in tqdm(img_files, desc="Processing test images"):
        img_path = os.path.join(test_dir, img_name)

        try:
            # Verify if file exists
            if not os.path.exists(img_path):
                print(f"Warning: File not found: {img_path}")
                continue

            # Check if file is empty
            if os.path.getsize(img_path) == 0:
                print(f"Warning: Empty file: {img_path}")
                continue

            # Try to open and verify the image
            img = load_img(img_path, target_size=target_size)

            # Convert to array and check if valid
            img_array = img_to_array(img)

            # Check if array has valid dimensions
            if img_array.shape != (*target_size, 3):
                print(f"Warning: Invalid image dimensions for {img_path}")
                continue

            # Normalize and append
            img_array = img_array / 255.0
            test_images.append(img_array)

            # Remove .jpg extension from image name
            clean_id = img_name.split('.')[0]
            test_ids.append(clean_id)

        except Exception as e:
            print(f"Error processing {img_path}: {str(e)}")
            continue

    if not test_images:
        raise ValueError("No valid images found in the test directory!")

    print(f"Successfully processed {len(test_images)} images")
    return np.array(test_images), test_ids

# Let's add some diagnostic information before processing
print(f"Test directory path: {TEST_DIR}")
print(f"Files in test directory: {os.listdir(TEST_DIR)[:5]} ... (showing first 5)")
print(f"Total files found: {len(os.listdir(TEST_DIR))}")

# Prepare test data with ordered images
x_test, test_ids = prepare_test_data(TEST_DIR)

# Predict labels for the test set
predictions = model.predict(x_test)
predicted_classes = np.argmax(predictions, axis=1)
predicted_labels = le.inverse_transform(predicted_classes)

# Create submission dataframe
submission_df = pd.DataFrame({
    'Id': test_ids,  # These IDs now don't have .jpg extension
    'Label': predicted_labels
})

# Add additional verification
print("\nSubmission DataFrame Info:")
print(submission_df.info())
print("\nFirst 5 rows:")
print(submission_df.head())
print("\nLast 5 rows:")
print(submission_df.tail())

# Save the submission file
submission_df.to_csv('submission.csv', index=False)
print("\nSubmission file saved as 'submission.csv'")